# ğŸ§ª Testing Guide for LLM Tournament Widget

This document provides comprehensive information about the testing infrastructure for the LLM Tournament Widget application.

## ğŸ“‹ **Testing Overview**

The application now includes comprehensive testing coverage for both backend and frontend components:

- **Backend**: Python FastAPI with pytest
- **Frontend**: React with Jest and Testing Library
- **Coverage**: API endpoints, data models, business logic, and UI components

## ğŸ **Backend Testing**

### **Test Structure**
```
backend/
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ conftest.py          # Pytest configuration and fixtures
â”‚   â”œâ”€â”€ test_api.py          # API endpoint tests
â”‚   â”œâ”€â”€ test_models.py       # Pydantic model tests
â”‚   â””â”€â”€ test_persistence.py  # Data persistence tests
â”œâ”€â”€ pytest.ini              # Pytest configuration
â”œâ”€â”€ run_tests.py            # Test runner script
â””â”€â”€ requirements.txt        # Includes testing dependencies
```

### **Dependencies Added**
- `pytest==7.4.0` - Testing framework
- `pytest-asyncio==0.21.1` - Async testing support
- `httpx==0.24.1` - HTTP client for testing FastAPI
- `pytest-cov==4.1.0` - Coverage reporting

### **Running Backend Tests**

#### **Option 1: Using the test runner script**
```bash
cd backend
python run_tests.py
```

#### **Option 2: Using pytest directly**
```bash
cd backend
python -m pytest tests/ -v --cov=. --cov-report=term-missing
```

#### **Option 3: Run specific test files**
```bash
cd backend
python run_tests.py test_api.py
python run_tests.py test_models.py
python run_tests.py test_persistence.py
```

#### **Option 4: Generate coverage report**
```bash
cd backend
python run_tests.py coverage
```

### **Backend Test Categories**

#### **1. API Endpoint Tests (`test_api.py`)**
- **Tournament API**: Create, read, delete tournaments
- **Results API**: Submit responses, get leaderboards
- **Utility Endpoints**: Schema endpoints, streaming tests

#### **2. Data Model Tests (`test_models.py`)**
- **Prompt Models**: Validation, required fields, optional fields
- **Tournament Models**: Status validation, prompt IDs
- **Result Models**: Score validation, field requirements

#### **3. Persistence Tests (`test_persistence.py`)**
- **File Operations**: Save/load JSON data
- **Error Handling**: Permission errors, disk full scenarios
- **Data Integrity**: Unicode support, large data handling

### **Backend Test Features**
- **Temporary Data Directories**: Each test uses isolated data
- **Mocked Dependencies**: Isolated testing without external services
- **Comprehensive Coverage**: Tests all major functionality
- **Error Scenarios**: Tests edge cases and failure modes

## âš›ï¸ **Frontend Testing**

### **Test Structure**
```
frontend/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ setupTests.ts        # Jest configuration and mocks
â”‚   â”œâ”€â”€ App.test.tsx         # Main App component tests
â”‚   â””â”€â”€ components/
â”‚       â””â”€â”€ CreateTournament.test.tsx  # Component tests
â”œâ”€â”€ run_tests.sh             # Test runner script
â””â”€â”€ package.json             # Includes testing dependencies
```

### **Dependencies Added**
- `@testing-library/react` - React component testing
- `@testing-library/jest-dom` - DOM testing utilities
- `@testing-library/user-event` - User interaction simulation

### **Running Frontend Tests**

#### **Option 1: Using the test runner script**
```bash
cd frontend
./run_tests.sh
```

#### **Option 2: Using npm directly**
```bash
cd frontend
npm test -- --coverage --watchAll=false
```

#### **Option 3: Run tests in watch mode**
```bash
cd frontend
npm test
```

### **Frontend Test Categories**

#### **1. App Component Tests (`App.test.tsx`)**
- **Component Rendering**: Header, forms, lists
- **User Interactions**: Creating tournaments, navigation
- **State Management**: Tournament selection, deletion
- **API Integration**: Data fetching, error handling

#### **2. Component Tests (`CreateTournament.test.tsx`)**
- **Form Validation**: Required fields, prompt validation
- **User Input**: Adding/removing prompts, form submission
- **API Calls**: Tournament creation, error handling
- **Form Reset**: Post-submission cleanup

### **Frontend Test Features**
- **Component Isolation**: Mocked dependencies and API calls
- **User Interaction**: Realistic user behavior simulation
- **Error Handling**: API failures, validation errors
- **State Management**: Component state changes and updates

## ğŸ¯ **Test Coverage**

### **Backend Coverage**
- **API Endpoints**: 100% of all endpoints tested
- **Data Models**: All Pydantic models validated
- **Business Logic**: Core functionality thoroughly tested
- **Error Handling**: Edge cases and failure scenarios

### **Frontend Coverage**
- **Components**: Main components and user flows
- **User Interactions**: Form submission, navigation
- **State Changes**: Component lifecycle and updates
- **API Integration**: Mocked API calls and responses

## ğŸš€ **Running All Tests**

### **Complete Test Suite**
```bash
# Backend tests
cd backend
python run_tests.py

# Frontend tests
cd ../frontend
./run_tests.sh
```

### **Continuous Integration Ready**
The test setup is designed to work in CI/CD environments:
- **Isolated Testing**: No external dependencies
- **Consistent Results**: Deterministic test outcomes
- **Coverage Reports**: HTML and terminal coverage output
- **Exit Codes**: Proper exit codes for CI systems

## ğŸ“Š **Coverage Reports**

### **Backend Coverage**
- **Terminal Output**: Real-time coverage during test execution
- **HTML Report**: Detailed coverage in `htmlcov/` directory
- **Missing Lines**: Identifies untested code paths

### **Frontend Coverage**
- **Terminal Output**: Coverage summary after test completion
- **HTML Report**: Detailed coverage in `coverage/` directory
- **File-by-File**: Component-level coverage breakdown

## ğŸ› **Debugging Tests**

### **Backend Debugging**
```bash
# Run with verbose output
python -m pytest tests/ -v -s

# Run specific test with debugger
python -m pytest tests/test_api.py::TestTournamentAPI::test_create_tournament_success -s

# Run with print statements visible
python -m pytest tests/ -s
```

### **Frontend Debugging**
```bash
# Run tests in watch mode for development
npm test

# Run specific test file
npm test -- CreateTournament.test.tsx

# Run with verbose output
npm test -- --verbose
```

## ğŸ“ **Writing New Tests**

### **Backend Test Guidelines**
1. **Use fixtures** from `conftest.py` for common setup
2. **Test both success and failure** scenarios
3. **Mock external dependencies** to isolate units
4. **Use descriptive test names** that explain the scenario
5. **Test edge cases** and error conditions

### **Frontend Test Guidelines**
1. **Mock API calls** to isolate component testing
2. **Test user interactions** using `userEvent`
3. **Verify component state** changes
4. **Test error scenarios** and loading states
5. **Use data-testid** attributes for reliable element selection

## ğŸ”§ **Test Configuration**

### **Backend Configuration (`pytest.ini`)**
- **Test Discovery**: Automatic test file detection
- **Coverage**: HTML and terminal coverage reports
- **Markers**: Custom test categories (unit, integration, api)
- **Output**: Verbose output with short tracebacks

### **Frontend Configuration (`setupTests.ts`)**
- **Jest DOM**: Custom DOM matchers
- **Axios Mocking**: API call isolation
- **Console Mocking**: Reduced test noise
- **Global Setup**: Consistent test environment

## ğŸ“ˆ **Performance Considerations**

### **Test Execution Time**
- **Backend**: ~5-10 seconds for full suite
- **Frontend**: ~10-15 seconds for full suite
- **Individual Tests**: <1 second each

### **Optimization Tips**
- **Parallel Execution**: Backend tests can run in parallel
- **Test Isolation**: Each test uses fresh data
- **Mocked Dependencies**: No external service calls
- **Efficient Fixtures**: Minimal setup/teardown overhead

## ğŸ‰ **Test Results**

With this testing infrastructure, the application now has:

- **Comprehensive Coverage**: All major functionality tested
- **Reliable Testing**: Deterministic test outcomes
- **Easy Maintenance**: Clear test structure and organization
- **CI/CD Ready**: Automated testing for deployment pipelines
- **Developer Experience**: Fast feedback on code changes

The testing setup provides confidence in code quality and makes the application more maintainable and reliable.
